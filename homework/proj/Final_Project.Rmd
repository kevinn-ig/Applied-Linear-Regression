---
title: "Final Project"
author: "Hunter Garrison, Kevin Smith, Reese Madsen, Giulio Martini"
date: "2024-04-26"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Section 1: Introduction

In this project, we are working with a dataset observing the prices of houses and corresponding attributes of each house in India. Below are the attributes of each house recorded in the dataset:

| Attribute               | Description                                     | Data Type |
|------------------|------------------------------------|------------------|
| [**price**]{.underline} | Price of the houses in rupees.                  | Integer   |
| area                    | Area of a house in square feet                  | Integer   |
| bedrooms                | Number of bedrooms in house                     | Integer   |
| bathrooms               | Number of bathrooms in house                    | Integer   |
| stories                 | Number of stories in house                      | Integer   |
| mainroad                | Whether or not house is connected to main road  | Boolean   |
| guestroom               | Whether or not house has a guest room           | Boolean   |
| basement                | Whether or not house has a basement             | Boolean   |
| hotwaterheating         | Whether or not house has a hot-water heater     | Boolean   |
| airconditioning         | Whether or not house has air conditioning       | Boolean   |
| parking                 | Number of parking spots at house                | Integer   |
| prefarea                | Whether or not the house is in a preferred area | Boolean   |
| furnishingstatus        | Furnishing status of the house                  | String    |

Below is the first 5 lines of our dataset:

```{r, echo=FALSE}
data <- read.csv('Housing.csv')
head(data, 5)
#colSums(is.na(data))
#hist(data$price, xlab="Price of House", ylab="Frequency", col="purple")
```

We are attempting to create a linear regression model that best explains the variance in the price of the houses using the predictors in our dataset. In this report, we will explore our data by finding collinearity, outliers, influential points, and by checking error assumption violations. We will also discover which variables are the most important by using model selection, and test OLS as well as GLS and WLS regressions.

# Section 2: Regression Analysis

## Model Diagnostics

In order to remove problematic data points from the data set, we must perform a series of test to identify them. To begin, we will examine our error assumptions to see if any have been violated using graphical methods.

#### Constant Variance Assumption

To verify that our model follows the constant variance assumption, we will be using the Breush-Pagan test at a significance level of $\alpha = .05$. Our null and alternative hypotheses are $H_0$: Homoscedastic errors and $H_1$: Heteroscedastic errors.

```{r, echo=FALSE, message = FALSE, warning = FALSE}
library(olsrr)
library(lmtest)
model = lm(price = ., data = data)

bptest(model)
```

The value of our test-statistic is 68.416 and our $p$-value is $1.569\times e^{-9}$. We reject the null hypothesis at the $\alpha = .05$ significance level and conclude that constant variance assumption is violated.

#### Normality Assumption

In order to check the normality assumption, we will be performing the Shapiro-Wilk test at the $\alpha = .05$ significance level. Our null and alternative hypotheses are $H_0$: The errors are normally distributed and $H_1$: The errors are not normally distributed.

```{r, echo=FALSE, message = FALSE, warning = FALSE}
shapiro.test(resid(model))
```

The value of our test-statistic is .95399 and our $p$-value is $5.31 \times e^{-12}$. We reject the null hypothesis at the $\alpha = .05$ significance level and conclude that errors are not normally distributed.

In order to try to correct for this, we will be using the Box-Cox method.

```{r, echo=FALSE, message = FALSE, warning = FALSE}
library(MASS)
bc = boxcox(model, lambda = seq(-0.25, 0.75, by = 0.05), plotit = TRUE)
bc$x[which.max(bc$y)]
```

According to the plot, and the print out, we can tell that $\hat{\lambda} = .05303$. So we will be performing an OLS regression with $\text{price}^{.05303}$ as the response and performing the Shapiro-Wilk test at the $\alpha = .05$ significance level.

```{r, echo=FALSE, message = FALSE, warning = FALSE}

model_bc = lm(price ^ 0.05303 ~ . , data = data)
bptest(model)
```

The value of our test-statistic is 68.416 and our $p$-value is $1.569 \times e^{-09}$. We reject the null hypothesis at the $\alpha = .05$ significance level and conclude that errors are still not normally distributed.

Since this model still does not comply with the normality assumption, we will be checking the RMSE and the percent variation in price explained by both models.

```{r, echo=FALSE, message = FALSE, warning = FALSE}
#RMSE of normal model
sqrt(mean((data$price - predict(model))^2))

# RMSE of the model with transformed response
sqrt(mean((data$price - predict(model_bc)^(1/0.05303))^2))

# sum of square totals for price
SST = sum((data$price - mean(data$price))^2)

# percent of variation in price explained by the model 
1 - sum((data$price - predict(model))^2) / SST

# percent of variation in species explained by the transformed model 
1 - sum((data$price - predict(model_bc)^(1/0.05303))^2) / SST
```

|       Model       |  RMSE   | \% Variation |
|:-----------------:|:-------:|:------------:|
|  Standard Model   | 1054129 |      68      |
| Transformed Model | 1041366 |      69      |

Since there is not much of a difference between the two models, we will be using the standard model as it is the simplest.

#### Linearity Assumption

```{r, echo=FALSE, message = FALSE, warning = FALSE}
ols_plot_added_variable(model)
```

It is clear that there is some linear relationship between the response and the numerical predictors (area, bathrooms, bedrooms, stories, parking).

#### Highly Influential Points

To check for highly influential points, we will be checking the cooks distances of the data.

```{r, echo=FALSE, message = FALSE, warning = FALSE}
length(which(cooks.distance(model) > 4 / length(cooks.distance(model))))
```

There are 45 highly influential points, we will now see if removing these points from the model will in any way correct our models assumption violations. First we will check the constant variance assumption at the $\alpha$ = .05 significance level.

```{r, echo=FALSE, message = FALSE, warning = FALSE}
noninfluential_ids = which(
    cooks.distance(model) <= 4/ length(cooks.distance(model)))

model_fix = lm(price ~ ., 
               data = data,
               subset = noninfluential_ids)

bptest(model_fix)
```

The value of our test-statistic is 31.26 and our $p$-value is .00309. We reject the null hypothesis at the $\alpha = .05$ significance level and conclude that constant variance assumption is violated.

Next we will check our constant variance assumption at the $\alpha$ = .05 significance level.

```{r, echo=FALSE, message = FALSE, warning = FALSE}
shapiro.test(resid(model_fix))
```

The value of our test-statistic is .98803 and our $p$-value is .000407. We reject the null hypothesis at the $\alpha = .05$ significance level and conclude that errors are not normally distributed.

We will also check the $R^2$ of both models,

```{r, echo=FALSE, message = FALSE, warning = FALSE}

summary(model)$r.squared

summary(model_fix)$r.squared
```

|                 Model                 | $R^2$ |
|:-------------------------------------:|:-----:|
|            Standard Model             | .682  |
| Model with Influential Points Removed | .756  |

It is clear that the Model with Influential Points removed is the preferred model as it explains almost 8% more of the observed variability in price with the predictors.

#### Outliers

To check for outliers we will be using the studentized residuals at the $\alpha$ = .05 significance level.

```{r, echo=FALSE, message = FALSE, warning = FALSE}
outlier_test_cutoff = function(model, alpha = 0.05) {
    n = length(resid(model))
    qt(alpha/(2 * n), df = df.residual(model) - 1, lower.tail = FALSE)
}

# vector of indices for observations deemed outliers.
cutoff = outlier_test_cutoff(model, alpha = 0.05)

length(which(abs(rstudent(model)) > cutoff))
```

We can see that there are only three outliers, which we are not too concerned about so we will be leaving them in the data.
